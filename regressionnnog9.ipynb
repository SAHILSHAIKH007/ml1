{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regressionnnog9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#AIM: To solve a regression problem using NN."
      ],
      "metadata": {
        "id": "JRHAEiHt-SpS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jmf_QpPG2B4G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "load_boston()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRY2_-Vq3Or-",
        "outputId": "b75c3158-edc8-433f-b909-6078327e5266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
              " 'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
              "         4.9800e+00],\n",
              "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
              "         9.1400e+00],\n",
              "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
              "         4.0300e+00],\n",
              "        ...,\n",
              "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
              "         5.6400e+00],\n",
              "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
              "         6.4800e+00],\n",
              "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
              "         7.8800e+00]]),\n",
              " 'data_module': 'sklearn.datasets.data',\n",
              " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
              "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
              " 'filename': 'boston_house_prices.csv',\n",
              " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
              "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
              "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
              "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
              "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
              "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
              "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
              "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
              "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
              "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
              "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
              "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
              "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
              "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
              "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
              "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
              "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
              "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
              "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
              "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
              "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
              "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
              "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
              "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
              "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
              "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
              "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
              "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
              "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
              "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
              "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
              "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
              "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
              "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
              "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
              "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
              "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
              "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
              "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
              "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
              "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
              "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
              "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
              "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
              "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
              "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "boston = load_boston()\n",
        "X = boston.data\n",
        "Y = boston.target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ09UaWY3OpD",
        "outputId": "4fadcf9f-db28-4a66-af07-5aaa5db137cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_boston "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxq0wmUS3OmP",
        "outputId": "9e61741b-743d-4523-c303-4af51f1783ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function sklearn.datasets._base.load_boston>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf-2IqUr3Ojl",
        "outputId": "41af100e-3800-4f1c-b8bd-625d62c26444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyLep0YP3Ogd",
        "outputId": "6fc1993b-4fbc-4df4-eaf0-87e32dd046ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
              "        4.9800e+00],\n",
              "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
              "        9.1400e+00],\n",
              "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
              "        4.0300e+00],\n",
              "       ...,\n",
              "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
              "        5.6400e+00],\n",
              "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
              "        6.4800e+00],\n",
              "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
              "        7.8800e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLOAvVsE3Od9",
        "outputId": "0fe4125b-f91d-4c52-d5e1-b9ae88adca21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFLJSyrT3hXY",
        "outputId": "bb18deef-c976-4e16-bf81-499cecd8c7f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506,)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvNGimxY3hUx",
        "outputId": "9cc3e7a7-3b6b-40df-d5fc-ea538ef8524a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
              "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
              "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
              "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
              "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
              "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
              "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
              "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
              "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
              "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
              "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
              "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
              "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
              "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
              "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
              "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
              "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
              "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
              "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
              "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
              "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
              "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
              "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
              "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
              "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
              "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
              "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
              "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
              "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
              "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
              "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
              "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
              "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
              "       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
              "       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
              "        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
              "       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
              "       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
              "        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
              "        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
              "       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
              "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
              "       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
              "       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
              "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
              "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc=StandardScaler()\n",
        "X_transform=sc.fit_transform(X)\n",
        "X_transform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeEs8b9F3hSQ",
        "outputId": "b3b1928b-eaf4-4129-da69-382a40f5c614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.41978194,  0.28482986, -1.2879095 , ..., -1.45900038,\n",
              "         0.44105193, -1.0755623 ],\n",
              "       [-0.41733926, -0.48772236, -0.59338101, ..., -0.30309415,\n",
              "         0.44105193, -0.49243937],\n",
              "       [-0.41734159, -0.48772236, -0.59338101, ..., -0.30309415,\n",
              "         0.39642699, -1.2087274 ],\n",
              "       ...,\n",
              "       [-0.41344658, -0.48772236,  0.11573841, ...,  1.17646583,\n",
              "         0.44105193, -0.98304761],\n",
              "       [-0.40776407, -0.48772236,  0.11573841, ...,  1.17646583,\n",
              "         0.4032249 , -0.86530163],\n",
              "       [-0.41500016, -0.48772236,  0.11573841, ...,  1.17646583,\n",
              "         0.44105193, -0.66905833]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X_transform,Y,test_size=0.2,random_state=0)"
      ],
      "metadata": {
        "id": "f0N7MZD177q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the libraries \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "#defining the input layer and first hidden layer\n",
        "model.add(Dense(units=13,input_dim=13,kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "#defining the second layer of the model\n",
        "#after first layer there is no need to specify input_dim as keras configure it automatically\n",
        "model.add(Dense(units=13,kernel_initializer='normal',activation='relu'))\n",
        "model.add(Dense(units=6,kernel_initializer='normal',activation='relu'))\n",
        "#model.add(Dense(units=2,kernel_initializer='normal',activation='relu'))\n",
        "#the output neuron is a single fully connected node\n",
        "# since we will be predicting a single number\n",
        "model.add(Dense(1,kernel_initializer='normal'))\n",
        "\n",
        "#compiling the model\n",
        "model.compile(loss='mean_squared_error',optimizer='adam')\n",
        "\n",
        "#Fitting the ANN to the training set\n",
        "model_1 = model.fit(X_train,Y_train,validation_data=(X_test,Y_test),batch_size=1,epochs=200,verbose=1)\n",
        " #Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration. The batch size can be one of three options: batch mode: where the batch size is equal to the total dataset thus making the iteration and epoch values equivalent\n",
        "#verbose basically means logs. It accepts values as 0/1. If you put Verbose value as 1 in ModelCheckPoint callback, after every epoch you will get a statement like Model improved from error 456 to 123 or model did not improve in this epoch."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSe8zXQO3hPv",
        "outputId": "e8e24fa9-b1e3-4b0c-caa7-a2cfd75dfabe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "404/404 [==============================] - 2s 3ms/step - loss: 412.5626 - val_loss: 116.7464\n",
            "Epoch 2/200\n",
            "404/404 [==============================] - 1s 3ms/step - loss: 46.6399 - val_loss: 58.9809\n",
            "Epoch 3/200\n",
            "404/404 [==============================] - 2s 4ms/step - loss: 30.9135 - val_loss: 51.6159\n",
            "Epoch 4/200\n",
            "404/404 [==============================] - 2s 4ms/step - loss: 27.1611 - val_loss: 48.8138\n",
            "Epoch 5/200\n",
            "404/404 [==============================] - 2s 5ms/step - loss: 24.7336 - val_loss: 46.1994\n",
            "Epoch 6/200\n",
            "404/404 [==============================] - 2s 5ms/step - loss: 22.9712 - val_loss: 43.1999\n",
            "Epoch 7/200\n",
            "404/404 [==============================] - 2s 5ms/step - loss: 21.0377 - val_loss: 39.9266\n",
            "Epoch 8/200\n",
            "404/404 [==============================] - 2s 4ms/step - loss: 19.6478 - val_loss: 36.9164\n",
            "Epoch 9/200\n",
            "404/404 [==============================] - 2s 5ms/step - loss: 18.3665 - val_loss: 34.4672\n",
            "Epoch 10/200\n",
            "404/404 [==============================] - 2s 5ms/step - loss: 17.4201 - val_loss: 33.1809\n",
            "Epoch 11/200\n",
            "404/404 [==============================] - 2s 4ms/step - loss: 16.4463 - val_loss: 30.8643\n",
            "Epoch 12/200\n",
            "404/404 [==============================] - 2s 4ms/step - loss: 15.5591 - val_loss: 31.1337\n",
            "Epoch 13/200\n",
            "404/404 [==============================] - 2s 6ms/step - loss: 15.0393 - val_loss: 29.5847\n",
            "Epoch 14/200\n",
            "404/404 [==============================] - 1s 3ms/step - loss: 14.5669 - val_loss: 28.3920\n",
            "Epoch 15/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 13.9990 - val_loss: 29.4180\n",
            "Epoch 16/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 13.7697 - val_loss: 27.5737\n",
            "Epoch 17/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 13.3435 - val_loss: 28.8298\n",
            "Epoch 18/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 12.9295 - val_loss: 26.3973\n",
            "Epoch 19/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 12.6599 - val_loss: 25.4533\n",
            "Epoch 20/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 12.3228 - val_loss: 23.8096\n",
            "Epoch 21/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 12.0580 - val_loss: 25.0383\n",
            "Epoch 22/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 11.9690 - val_loss: 24.6954\n",
            "Epoch 23/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 11.6288 - val_loss: 23.9767\n",
            "Epoch 24/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 11.4950 - val_loss: 23.8295\n",
            "Epoch 25/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 11.0898 - val_loss: 22.5895\n",
            "Epoch 26/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 10.7807 - val_loss: 21.6349\n",
            "Epoch 27/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 10.8845 - val_loss: 21.8306\n",
            "Epoch 28/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 10.6176 - val_loss: 21.3745\n",
            "Epoch 29/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 10.4236 - val_loss: 22.7370\n",
            "Epoch 30/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 10.4671 - val_loss: 22.0042\n",
            "Epoch 31/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 10.0517 - val_loss: 20.6739\n",
            "Epoch 32/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 9.9926 - val_loss: 22.2186\n",
            "Epoch 33/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 9.7689 - val_loss: 20.8008\n",
            "Epoch 34/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 9.6764 - val_loss: 20.1469\n",
            "Epoch 35/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 9.1995 - val_loss: 20.4231\n",
            "Epoch 36/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 9.5596 - val_loss: 20.7737\n",
            "Epoch 37/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 9.3657 - val_loss: 22.3260\n",
            "Epoch 38/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 9.2913 - val_loss: 20.4943\n",
            "Epoch 39/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 9.1364 - val_loss: 19.8039\n",
            "Epoch 40/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 8.8024 - val_loss: 20.6971\n",
            "Epoch 41/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 8.8988 - val_loss: 19.7602\n",
            "Epoch 42/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 8.9309 - val_loss: 19.9242\n",
            "Epoch 43/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 8.4917 - val_loss: 19.5492\n",
            "Epoch 44/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 8.6581 - val_loss: 20.0467\n",
            "Epoch 45/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 8.5568 - val_loss: 20.2223\n",
            "Epoch 46/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 8.4512 - val_loss: 19.9916\n",
            "Epoch 47/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 8.3557 - val_loss: 19.7589\n",
            "Epoch 48/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 8.1936 - val_loss: 19.0984\n",
            "Epoch 49/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 8.2906 - val_loss: 18.9994\n",
            "Epoch 50/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.9520 - val_loss: 18.7398\n",
            "Epoch 51/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 8.2597 - val_loss: 19.7887\n",
            "Epoch 52/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 8.1267 - val_loss: 19.9284\n",
            "Epoch 53/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.9431 - val_loss: 19.1687\n",
            "Epoch 54/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.8259 - val_loss: 18.8967\n",
            "Epoch 55/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.9193 - val_loss: 19.2865\n",
            "Epoch 56/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.7520 - val_loss: 20.1018\n",
            "Epoch 57/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.9110 - val_loss: 19.7251\n",
            "Epoch 58/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.7933 - val_loss: 19.4158\n",
            "Epoch 59/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.6814 - val_loss: 18.4285\n",
            "Epoch 60/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.4443 - val_loss: 19.4450\n",
            "Epoch 61/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.6520 - val_loss: 18.7511\n",
            "Epoch 62/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.3813 - val_loss: 18.7401\n",
            "Epoch 63/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.6583 - val_loss: 18.0269\n",
            "Epoch 64/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.5892 - val_loss: 19.4395\n",
            "Epoch 65/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.5884 - val_loss: 18.8980\n",
            "Epoch 66/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.4636 - val_loss: 18.9712\n",
            "Epoch 67/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.5018 - val_loss: 20.7445\n",
            "Epoch 68/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.2071 - val_loss: 21.2513\n",
            "Epoch 69/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.3685 - val_loss: 18.7302\n",
            "Epoch 70/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.1238 - val_loss: 21.3233\n",
            "Epoch 71/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.4587 - val_loss: 19.6020\n",
            "Epoch 72/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.3379 - val_loss: 20.8063\n",
            "Epoch 73/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.1883 - val_loss: 20.4238\n",
            "Epoch 74/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.2429 - val_loss: 19.1762\n",
            "Epoch 75/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.3370 - val_loss: 19.3555\n",
            "Epoch 76/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.1996 - val_loss: 19.0614\n",
            "Epoch 77/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.2608 - val_loss: 20.6928\n",
            "Epoch 78/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.9287 - val_loss: 18.6485\n",
            "Epoch 79/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.0986 - val_loss: 19.8472\n",
            "Epoch 80/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.0624 - val_loss: 19.3644\n",
            "Epoch 81/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 7.0942 - val_loss: 19.7105\n",
            "Epoch 82/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.8228 - val_loss: 18.6245\n",
            "Epoch 83/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.8668 - val_loss: 18.3541\n",
            "Epoch 84/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.8884 - val_loss: 17.9751\n",
            "Epoch 85/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.7944 - val_loss: 20.8139\n",
            "Epoch 86/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.9791 - val_loss: 19.6105\n",
            "Epoch 87/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.7584 - val_loss: 20.6021\n",
            "Epoch 88/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.8148 - val_loss: 20.6962\n",
            "Epoch 89/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.7847 - val_loss: 18.8090\n",
            "Epoch 90/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.8907 - val_loss: 20.9978\n",
            "Epoch 91/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.6466 - val_loss: 18.1020\n",
            "Epoch 92/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.9161 - val_loss: 19.2165\n",
            "Epoch 93/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.6727 - val_loss: 20.6199\n",
            "Epoch 94/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.6574 - val_loss: 18.6009\n",
            "Epoch 95/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.6860 - val_loss: 20.2026\n",
            "Epoch 96/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.7688 - val_loss: 20.0476\n",
            "Epoch 97/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.5897 - val_loss: 20.8510\n",
            "Epoch 98/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.7267 - val_loss: 20.7563\n",
            "Epoch 99/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.5381 - val_loss: 20.5355\n",
            "Epoch 100/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.5843 - val_loss: 18.1836\n",
            "Epoch 101/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.6071 - val_loss: 18.2813\n",
            "Epoch 102/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.6020 - val_loss: 19.7981\n",
            "Epoch 103/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.4373 - val_loss: 18.3385\n",
            "Epoch 104/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.6001 - val_loss: 19.3326\n",
            "Epoch 105/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.6389 - val_loss: 19.2196\n",
            "Epoch 106/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.4205 - val_loss: 19.5992\n",
            "Epoch 107/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.6831 - val_loss: 19.0578\n",
            "Epoch 108/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.6067 - val_loss: 18.5393\n",
            "Epoch 109/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.3957 - val_loss: 20.6335\n",
            "Epoch 110/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.4966 - val_loss: 19.3310\n",
            "Epoch 111/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.3978 - val_loss: 18.5663\n",
            "Epoch 112/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.4954 - val_loss: 18.8386\n",
            "Epoch 113/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.3113 - val_loss: 19.8615\n",
            "Epoch 114/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.3988 - val_loss: 18.9863\n",
            "Epoch 115/200\n",
            "404/404 [==============================] - 1s 3ms/step - loss: 6.3971 - val_loss: 18.5335\n",
            "Epoch 116/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.4074 - val_loss: 19.3137\n",
            "Epoch 117/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.3879 - val_loss: 19.9038\n",
            "Epoch 118/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.2451 - val_loss: 18.5408\n",
            "Epoch 119/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.2339 - val_loss: 19.3172\n",
            "Epoch 120/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.2742 - val_loss: 18.4971\n",
            "Epoch 121/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.3229 - val_loss: 18.3930\n",
            "Epoch 122/200\n",
            "404/404 [==============================] - 1s 3ms/step - loss: 6.2765 - val_loss: 19.3446\n",
            "Epoch 123/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.4934 - val_loss: 19.2291\n",
            "Epoch 124/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.3255 - val_loss: 19.5014\n",
            "Epoch 125/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.3835 - val_loss: 19.0165\n",
            "Epoch 126/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.2076 - val_loss: 18.9676\n",
            "Epoch 127/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.3581 - val_loss: 18.5038\n",
            "Epoch 128/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.2977 - val_loss: 18.7702\n",
            "Epoch 129/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.2029 - val_loss: 19.5620\n",
            "Epoch 130/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.1242 - val_loss: 17.7091\n",
            "Epoch 131/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.2630 - val_loss: 17.7377\n",
            "Epoch 132/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.3017 - val_loss: 19.3752\n",
            "Epoch 133/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.0981 - val_loss: 18.6733\n",
            "Epoch 134/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.2652 - val_loss: 17.6677\n",
            "Epoch 135/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.3291 - val_loss: 18.4532\n",
            "Epoch 136/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.1911 - val_loss: 18.9903\n",
            "Epoch 137/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.2534 - val_loss: 18.2904\n",
            "Epoch 138/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.1626 - val_loss: 18.1818\n",
            "Epoch 139/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.1987 - val_loss: 19.0544\n",
            "Epoch 140/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.1320 - val_loss: 18.3161\n",
            "Epoch 141/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.1250 - val_loss: 18.8339\n",
            "Epoch 142/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.1147 - val_loss: 18.9128\n",
            "Epoch 143/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.9721 - val_loss: 18.1484\n",
            "Epoch 144/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.0852 - val_loss: 19.9746\n",
            "Epoch 145/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.0405 - val_loss: 17.7950\n",
            "Epoch 146/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.0982 - val_loss: 20.3553\n",
            "Epoch 147/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.0625 - val_loss: 18.3195\n",
            "Epoch 148/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.1845 - val_loss: 18.9094\n",
            "Epoch 149/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.9500 - val_loss: 18.1537\n",
            "Epoch 150/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.9703 - val_loss: 20.8211\n",
            "Epoch 151/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.8795 - val_loss: 20.4005\n",
            "Epoch 152/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.2281 - val_loss: 18.0900\n",
            "Epoch 153/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.7561 - val_loss: 19.9694\n",
            "Epoch 154/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.0339 - val_loss: 18.9254\n",
            "Epoch 155/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.9180 - val_loss: 18.3551\n",
            "Epoch 156/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.0382 - val_loss: 17.9971\n",
            "Epoch 157/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.0138 - val_loss: 18.9858\n",
            "Epoch 158/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.0266 - val_loss: 19.0486\n",
            "Epoch 159/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.7798 - val_loss: 18.0586\n",
            "Epoch 160/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.7808 - val_loss: 18.5526\n",
            "Epoch 161/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.1283 - val_loss: 18.7101\n",
            "Epoch 162/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.9245 - val_loss: 18.5428\n",
            "Epoch 163/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.9567 - val_loss: 19.6110\n",
            "Epoch 164/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.9226 - val_loss: 20.1731\n",
            "Epoch 165/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 6.0700 - val_loss: 17.9174\n",
            "Epoch 166/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.8925 - val_loss: 18.7339\n",
            "Epoch 167/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.8911 - val_loss: 18.8871\n",
            "Epoch 168/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.8641 - val_loss: 19.4502\n",
            "Epoch 169/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.8796 - val_loss: 19.2959\n",
            "Epoch 170/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.8808 - val_loss: 20.6016\n",
            "Epoch 171/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.7318 - val_loss: 18.5561\n",
            "Epoch 172/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.8623 - val_loss: 19.2638\n",
            "Epoch 173/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.9418 - val_loss: 18.6906\n",
            "Epoch 174/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.8782 - val_loss: 19.9786\n",
            "Epoch 175/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.8354 - val_loss: 18.1087\n",
            "Epoch 176/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.7036 - val_loss: 18.5526\n",
            "Epoch 177/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.9255 - val_loss: 18.4293\n",
            "Epoch 178/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.7367 - val_loss: 19.1576\n",
            "Epoch 179/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.7640 - val_loss: 19.0771\n",
            "Epoch 180/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.6715 - val_loss: 20.4940\n",
            "Epoch 181/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.6117 - val_loss: 19.7022\n",
            "Epoch 182/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.7697 - val_loss: 18.6043\n",
            "Epoch 183/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.7426 - val_loss: 18.0179\n",
            "Epoch 184/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.5478 - val_loss: 17.8605\n",
            "Epoch 185/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.5548 - val_loss: 19.1957\n",
            "Epoch 186/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.6706 - val_loss: 20.1929\n",
            "Epoch 187/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.5689 - val_loss: 20.6363\n",
            "Epoch 188/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.5349 - val_loss: 17.8397\n",
            "Epoch 189/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.6925 - val_loss: 18.1729\n",
            "Epoch 190/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.6294 - val_loss: 19.1582\n",
            "Epoch 191/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.5544 - val_loss: 18.3929\n",
            "Epoch 192/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.5945 - val_loss: 20.6770\n",
            "Epoch 193/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.6534 - val_loss: 19.8763\n",
            "Epoch 194/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.5906 - val_loss: 19.4234\n",
            "Epoch 195/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.7069 - val_loss: 18.9273\n",
            "Epoch 196/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.6369 - val_loss: 18.7444\n",
            "Epoch 197/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.6374 - val_loss: 19.2668\n",
            "Epoch 198/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.6375 - val_loss: 18.1049\n",
            "Epoch 199/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.3836 - val_loss: 20.1971\n",
            "Epoch 200/200\n",
            "404/404 [==============================] - 1s 2ms/step - loss: 5.5861 - val_loss: 19.1591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_range = range(0,200)\n",
        "fig = plt.figure()\n",
        "#a1 = fig.add_axes([0,0,1,1])\n",
        "plt.plot(epoch_range, model_1.history['loss'])\n",
        "plt.plot(epoch_range, model_1.history['val_loss'])\n",
        "plt.title('model Error')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "#a1.set_ylim(0,1)\n",
        "#plt.legend(['Train','Val'], loc = 'upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "9_Ml5w4oxrfa",
        "outputId": "29a5c352-c7d2-4a57-b6b1-a8751bae7036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred_mlr = model.predict(X_test)"
      ],
      "metadata": {
        "id": "vJSCG708xsxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "meanAbErr = metrics.mean_absolute_error(Y_test, Y_pred_mlr)\n",
        "meanSqErr = metrics.mean_squared_error(Y_test, Y_pred_mlr)\n",
        "rootMeanSqErr = np.sqrt(metrics.mean_squared_error(Y_test, Y_pred_mlr))\n",
        "#print('R squared: {:.2f}'.format(model.score(x,y)*100))\n",
        "print('Mean Absolute Error:', meanAbErr)\n",
        "print('Mean Square Error:', meanSqErr)\n",
        "print('Root Mean Square Error:', rootMeanSqErr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPh1833UxsnV",
        "outputId": "a5384fe8-708e-4961-8a14-43019e717f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 2.93269061200759\n",
            "Mean Square Error: 19.159144872841086\n",
            "Root Mean Square Error: 4.377116045165022\n"
          ]
        }
      ]
    }
  ]
}